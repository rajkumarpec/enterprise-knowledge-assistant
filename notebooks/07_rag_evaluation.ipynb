{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0241d701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PROJECT_ROOT: c:\\Users\\rajku\\Desktop\\enterprise_knowledge_assistant\n",
      "ðŸ“¦ Config loaded from: c:\\Users\\rajku\\Desktop\\enterprise_knowledge_assistant\\config.py\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”’ BULLETPROOF project + config import cell\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Detect project root by searching for config.py\n",
    "current = Path.cwd()\n",
    "\n",
    "PROJECT_ROOT = None\n",
    "for parent in [current] + list(current.parents):\n",
    "    if (parent / \"config.py\").exists():\n",
    "        PROJECT_ROOT = parent\n",
    "        break\n",
    "\n",
    "if PROJECT_ROOT is None:\n",
    "    raise RuntimeError(\"âŒ config.py not found in any parent directory\")\n",
    "\n",
    "# Add project root to sys.path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "# Now import config\n",
    "import config\n",
    "\n",
    "print(\"âœ… PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"ðŸ“¦ Config loaded from:\", config.__file__)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2699a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from src.vector_store.faiss_store import load_vector_store\n",
    "from src.rag.citation import retrieve_with_citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee4ba2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vector store loaded from:\n",
      "C:\\Users\\rajku\\Desktop\\enterprise_knowledge_assistant\\data\\processed\\vector_store.pkl\n",
      "âœ… Loaded 1797 vectors\n"
     ]
    }
   ],
   "source": [
    "# Load Models & Vector Store\n",
    "\n",
    "# Embedding model (same as retrieval)\n",
    "embedding_model = SentenceTransformer(config.EMBEDDING_MODEL_NAME)\n",
    "\n",
    "# Vector store\n",
    "faiss_index, documents = load_vector_store(config.VECTOR_STORE_PATH)\n",
    "\n",
    "print(f\"âœ… Loaded {faiss_index.ntotal} vectors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2602c790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Inputs (From Step-6)----> Paste one real answer generated earlier.\n",
    "\n",
    "query = \"thermal behaviour\"\n",
    "\n",
    "answer = \"\"\"\n",
    "--------------------------------------------------------------------------------[1] (distance=1.0176)\n",
    "into the mechanical behaviour under simulated operational conditions. The data illustrated in \n",
    "Figures 20 and 21 elucidate these strain rate responses throughout the thermal cycle, particularly \n",
    "noting significant inflexion points at 2,000 and 14,000 seconds within the cycle timeline.\n",
    "\n",
    "[2] (distance=1.0396)\n",
    "density increases as the temperature increases. The materials exhibit different rates of increase, with \n",
    "Sn63Pb37 having a more pronounced rise than the others. This kind of analysis is useful in selecting \n",
    "materials for thermal applications where expansion due to heat can affect performance.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db91db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Context Again (Ground Truth)\n",
    "\n",
    "retrieved_chunks = retrieve_with_citations(\n",
    "    query=query,\n",
    "    model=embedding_model,\n",
    "    index=faiss_index,\n",
    "    documents=documents,\n",
    "    top_k=config.TOP_K\n",
    ")\n",
    "\n",
    "retrieved_text = \" \".join([c[\"content\"] for c in retrieved_chunks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0ea55a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRIC 1 â€” Citation Coverage\n",
    "# Question: Did the answer actually use citations?\n",
    "\n",
    "def citation_coverage(answer: str) -> float:\n",
    "    citations = re.findall(r\"\\[\\d+\\]\", answer)\n",
    "    return len(set(citations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa154d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Citation count: 2\n"
     ]
    }
   ],
   "source": [
    "num_citations = citation_coverage(answer)\n",
    "print(\"ðŸ“Œ Citation count:\", num_citations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30229438",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "0 â†’ âŒ ungrounded\n",
    "\n",
    ">=1 â†’ âœ… grounded answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df7e4378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRIC 2 â€” Faithfulness Score (MOST IMPORTANT)\n",
    "## Question: Does the answer content actually come from retrieved context?\n",
    "## We use semantic similarity between:   answer and retrieved context\n",
    "\n",
    "def faithfulness_score(\n",
    "    answer: str,\n",
    "    retrieved_context: str,\n",
    "    model: SentenceTransformer\n",
    ") -> float:\n",
    "\n",
    "    emb_answer = model.encode(answer, convert_to_tensor=True)\n",
    "    emb_context = model.encode(retrieved_context, convert_to_tensor=True)\n",
    "\n",
    "    score = util.cos_sim(emb_answer, emb_context).item()\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2772fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Faithfulness score: 0.782\n"
     ]
    }
   ],
   "source": [
    "faith_score = faithfulness_score(answer, retrieved_text, embedding_model)\n",
    "print(f\"ðŸ§  Faithfulness score: {faith_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0993fec0",
   "metadata": {},
   "source": [
    "# Interpretation (Very Important)\n",
    "Score\tMeaning\n",
    "> 0.75\tâœ… Highly faithful\n",
    "\n",
    "0.55 â€“ 0.75\tâš  Partially faithful\n",
    "\n",
    "< 0.55\tâŒ Likely hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4bae46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval Quality\n",
    "## Question: Were retrieved chunks actually relevant to the query?\n",
    "\n",
    "## We measure similarity between: query and each retrieved chunk\n",
    "\n",
    "def retrieval_relevance(\n",
    "    query: str,\n",
    "    retrieved_chunks,\n",
    "    model\n",
    "):\n",
    "    q_emb = model.encode(query, convert_to_tensor=True)\n",
    "    scores = []\n",
    "\n",
    "    for c in retrieved_chunks:\n",
    "        c_emb = model.encode(c[\"content\"], convert_to_tensor=True)\n",
    "        scores.append(util.cos_sim(q_emb, c_emb).item())\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8d575cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 relevance: 0.491\n",
      "Chunk 2 relevance: 0.480\n",
      "Chunk 3 relevance: 0.442\n",
      "Chunk 4 relevance: 0.420\n"
     ]
    }
   ],
   "source": [
    "relevance_scores = retrieval_relevance(\n",
    "    query, retrieved_chunks, embedding_model\n",
    ")\n",
    "\n",
    "for i, score in enumerate(relevance_scores, 1):\n",
    "    print(f\"Chunk {i} relevance: {score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd4c0b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== RAG EVALUATION SUMMARY =====\n",
      "Query: thermal behaviour\n",
      "Citations used: 2\n",
      "Faithfulness score: 0.782\n",
      "Avg retrieval relevance: 0.458\n"
     ]
    }
   ],
   "source": [
    "# FINAL EVALUATION SUMMARY CELL\n",
    "\n",
    "print(\"===== RAG EVALUATION SUMMARY =====\")\n",
    "print(\"Query:\", query)\n",
    "print(\"Citations used:\", num_citations)\n",
    "print(f\"Faithfulness score: {faith_score:.3f}\")\n",
    "print(f\"Avg retrieval relevance: {sum(relevance_scores)/len(relevance_scores):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a0f788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
