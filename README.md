## ğŸ“š Enterprise Knowledge Assistant
Local Multi-Document RAG System with Evaluation & Citations

ğŸ”¹ Overview

This project implements a fully local Retrieval-Augmented Generation (RAG) system that answers user questions from multiple PDF documents.
It combines semantic search (FAISS), local LLMs via Ollama, citation-aware answers, and evaluation metrics to ensure reliability and trust.

The system follows production-style modular architecture and supports incremental document updates without rebuilding the entire vector store.

## ğŸ”¹ Key Capabilities

âœ… Multi-PDF ingestion with document & page metadata

âœ… Robust text chunking with overlap

âœ… Sentence-Transformer embeddings

âœ… FAISS vector store (initial + incremental indexing)

âœ… Citation-aware RAG responses

âœ… Faithfulness & retrieval relevance evaluation

âœ… Local LLM inference using Ollama

âœ… Streamlit UI with model switching

âœ… Fully offline & privacy-preserving

## ğŸ”¹ Architecture

PDF Documents
     â†“
Ingestion (metadata enrichment)
     â†“
Chunking
     â†“
Embeddings (Sentence Transformers)
     â†“
FAISS Vector Store
     â†“
Retriever
     â†“
LLM (Ollama)
     â†“
Answer + Citations
     â†“
Evaluation (Faithfulness & Relevance)


## ğŸ”¹ Tech Stack

Language: Python

Embeddings: Sentence-Transformers (all-MiniLM-L6-v2)

Vector DB: FAISS

LLMs: LLaMA 3.2 / Phi-3 Mini (via Ollama)

Frameworks: LangChain

UI: Streamlit

## ğŸ”¹ Project Structure

src/
 â”œâ”€â”€ ingestion/        # PDF loading & metadata enrichment
 â”œâ”€â”€ chunking/         # Text chunking logic
 â”œâ”€â”€ embeddings/       # Embedding generation
 â”œâ”€â”€ vector_store/     # FAISS build, save, load, update
 â”œâ”€â”€ rag/              # Retrieval, prompt, citations
data/
 â”œâ”€â”€ documents/        # Input PDFs
 â”œâ”€â”€ processed/        # Saved vector store
notebooks/
 â”œâ”€â”€ ingestion
 â”œâ”€â”€ chunking
 â”œâ”€â”€ embeddings
 â”œâ”€â”€ faiss
 â”œâ”€â”€ evaluation
app.py                 # Streamlit UI
config.py              # Central configuration


## ğŸ”¹ How to Run
### 1ï¸âƒ£ Install dependencies

pip install -r requirements.txt

### 2ï¸âƒ£ Run Ollama (local LLM)

ollama run llama3.2:3b

### 3ï¸âƒ£ Build vector store (initial)

Run notebooks in order:

Ingestion

Chunking

Embeddings

FAISS initial build

### 4ï¸âƒ£ Launch UI
streamlit run app.py


## ğŸ”¹ Incremental Document Update

New PDFs can be added without rebuilding the entire index:

Add PDF to data/documents/

Run incremental FAISS update notebook

Restart Streamlit

## ğŸ”¹ Evaluation & Trust

The system includes an evaluation layer to reduce hallucinations:

Citation coverage â€“ verifies source grounding

Faithfulness score â€“ semantic similarity between answer and retrieved context

Retrieval relevance â€“ queryâ€“chunk similarity scores

## ğŸ”¹ Privacy & Security

Fully local execution

No cloud APIs

Documents never leave the machine

Suitable for confidential or institutional documents.

## ğŸ”¹ Status

âœ” Complete
âœ” Resume-ready
âœ” Interview-ready

ğŸ”¹ Author

Raj Kumar
Assistant Professor, G D Goenka University
Data Science | Machine Learning | GenAI